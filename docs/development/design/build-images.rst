Build Images
============

This document describes how Read the Docs uses the Docker Build Images and how they are named.
Besides, it proposes a new way to create and name them to allow
sharing as many image layers as possible to support more customization while keeping the stability.


Introduction
------------

We use Docker images to build user's documentation.
Each time a build is triggered, one of our VMs picks the task
and go through different steps:

#. run some application code to spin up a Docker image into a container
#. execute git inside the container to clone the repository
#. analyze and parse files from the repository *outside* the container
#. create the environment and install docs' dependencies inside the container
#. execute build commnands inside the container
#. push the output generated by builds commands to the storage


*All* those steps depends on specific commands versions: ``git``, ``python``, ``virtualenv``, ``conda``, etc.
Currently, we are pinning only a few of them in our Docker images and that have caused issues
when re-deploying these images with bugfixes: **the images are not reproducible in time**.

.. note::

   The repoducibility of the images will be fixed once
   https://github.com/readthedocs/readthedocs-docker-images/pull/145 and
   https://github.com/readthedocs/readthedocs-docker-images/pull/146
   get merged.

To allow users to pin the image we ended up exposing three images: ``stable``, ``latest`` and ``testing``.
With that naming, we were able to bugfix issues and add more features
on each image without asking the users to change the image selected in their config file.

Then, when a completely different image appeared and after testing ``testing`` image enough,
we discarded ``stable``, old ``latest`` became the new ``stable`` and old ``testing`` became the new ``latest``.
This produced issues to people pinning their images to any of these names because after this change,
*we changed all the images for all the users* and many build issues arrised!


Goals
-----

* release a completely new Docker image without forcing users to change their pinned image
* allow users to stick with an image "forever" (~years)
* use a ``base`` image with the dependencies that don't change frequently (OS and base requirements)
* reduce size on builder VM disks by sharing Docker image layers
* deprecate ``stable``, ``latest`` and ``testing``
* allow use custom images for particular users/customers by sharing most layers


New build image structure
-------------------------

.. Some of this is borrowed from CircleCI image versioning

``readthedocs/build:base``
    Alias of ``readthedocs/build:base-ubuntu20``, used to abstract concept of OS
    away from end users where it's not neccessary.

    Base images also include:

    ``readthedocs/build:base-ubuntu20``
        Base for next gen prod images. Synonymous with current concept of ``latest``

    ``readthedocs/build:base-ubuntu18``
        Base for previous gen prod images. Synonymous with concept of ``stable``

    Base images include:

    * labels
    * environment variables
    * system dependencies
    * install requirements
    * PDF/LaTeX dependencies
    * Limited user requirements
      * plantuml, imagemagick, rsgv-convert, swig
      * sphinx-js dependencies
    * UID and GID

``readthedocs/build:python``
    Installs latest release of Python

``readthedocs/build:python39``
    Installs 3.9 release of Python

``readthedocs/build:python39-ubuntu18``
    Installs 3.9 release of Python on Ubuntu 18.04 base
    (``readthedocs/build:base-ubuntu18``)

``readthedocs/build:python27``
    Installs 3.9 release of Python

``readthedocs/build:conda12``
    Installs 1.2 release of Conda

``readthedocs/build:node12``
    Installs Node 12.x release

``readthedocs/build:python+node``
    Installs latest Python and latest Node

``readthedocs/build:python39+node12``
    Installs Python 3.9 and Node 12.0

``readthedocs/build:python36+node8-ubuntu18``
    Installs Python 3.6 and Node 8 on Ubuntu 18.04 base


Building
~~~~~~~~

This process would be automated. CircleCI does something similar but with far
more complexity than we need:

https://github.com/circleci/circleci-images

We would use envvars or generate a pile of Dockerfiles. Dockerfiles would be
consumed by whatever process we are using to build images with intermediate
layers.

Custom images
-------------

To handle custom image requirements, we will support custom Docker images. To
start, this is under feature flag. This will reduce the amount of maintenance by
limiting the number of dependencies users ask us to maintain.

#. User builds image with base image of ``readthedocs/build:base`` or
   ``readthedocs/build:python`` etc.
  * On container start, Docker fetches the missing image
  * The user is responsible for versioning their own image in a way that it is
    updated on the build servers when it does not exist.
  * We need to limit the number of users that can do this until we know what
    disk usage looks like.

Updating versions over time
---------------------------

How do we add/upgrade a Python version?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We edit the ``python39-ubuntu18`` and ``python39-ubuntu20`` Dockerfile. There is
no backporting and risk due to change is limited to these two images.

How do we upgrade system versions?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We usually don't upgrade these dependencies unless we upgrade the Ubuntu version.
So, they will be only upgraded when we go from Ubuntu 18.04 LTS to Ubuntu 20.04 LTS for example.

Examples of these versions are:

* doxygen
* git
* subversion
* pandoc
* nodejs / npm
* swig
* rust


How do we add an extra requirement?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We don't. Users will manage weird requirements themselves by using custom
images. If the majority of users do not need the requirement, we shouldn't bloat
the base images with it.


How do we remove an old Python version?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At some point an old version of Python will be deprecated (eg. 3.4) and will be removed from our Docker images.
These versions should only be removed when the OS in the ``base`` is upgraded (eg. from ``ubuntu20`` to ``ubuntu22``).


Deprecation plan
----------------

It seems we have ~50Gb free on builders disks.
Considering that the new images will be sized approximately (built locally as test):

* ``base``: ~2.5Gb
* ``nopdf``: ~5.5Gb
* ``pdf``: ~1.5Gb

which is about ~10Gb in total, we will still have space to support multiple custom images.

We could keep ``stable``, ``latest`` and ``testing`` for some time without worry too much.
New projects shouldn't be able to select these images and they will be forced to use ``ubuntu20``
or any other custom image.

We may want to keep the three latest Ubuntu LTS releases available in production.
At the moment of writing this they are:

* Ubuntu 16.04 LTS (we are not using it anymore)
* Ubuntu 18.04 LTS (our ``stable``, ``latest`` and ``testing`` images)
* Ubuntu 20.04 LTS (our new ``ubuntu20``)

Once Ubuntu 22.04 LTS is released, we should deprecate Ubuntu 16.04 LTS,
and give users 6 months to migrate to a newer image.
User with custom images based on Ubuntu 16.04 LTS will be forced to migrate as well.


Conclusion
----------

I don't think we need to differentiate the images by its state (stable, latest, testing)
but by its main base difference: OS. The version of the OS will change many library versions,
LaTeX dependencies, basic required commands like git and more,
that doesn't seem to be useful to have the same OS version with different states.

The config key ``python.version`` will dictate the image used and most users
won't need to specify an image to use at all. Only for select cases will users
need to specify both.

Custom images is something that needs more exploration still,
but both proposals seem doable in weeks as an initial proof of concept.
